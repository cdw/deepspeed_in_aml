{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefb569c",
   "metadata": {},
   "source": [
    "# Walk through training a model on AML using Deepspeed, registering the resulting model\n",
    "\n",
    "**Goals**: this notebook demonstrates how to fine-tune an NLP model on a cluster on Deepspeed within AML. It uses a minimal external training script that is intended to be used as a starting point for adaptation to other workflows.\n",
    "\n",
    "This combines and supplements the work done in stories UM99, UM109, and UM159 for easy reference in a single location. It provides a minimal example of configuring and launching a model training run on AzureML using Deepspeed as an accelerator for distributed training. Here we'll\n",
    "\n",
    "- Import a configuration file that sets parameters for our training run\n",
    "- Connect to AML and locate the Deepspeed environment we've built there\n",
    "- Configure a run, using a minimal external training script\n",
    "- Submit our run to a compute cluster\n",
    "- Look at a completed run and the model it registered \n",
    "\n",
    "First we will import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05603b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import azureml.core\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc50fbc",
   "metadata": {},
   "source": [
    "## Using an external config.yml\n",
    "\n",
    "The `src/config.yml` file contains most of the settings that will need to be customized for a new job type. Reading through it gives a sense of how to configure a run, what sort of information to think about when defining a new experiment. Additionally, by storing values for this notebook and `train.py` in a config file (rather than passing them by argument) we make them far easier to track. Our `train.py` script will ensure that the input config file is copied to the run outputs, preserving it with the run for later reference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1551894",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('src/config.yml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8ce9a",
   "metadata": {},
   "source": [
    "## Connect to the workspace and environment \n",
    "\n",
    "The AzureML workspace hosts our compute, dataset, future models, and environment. We instantiate a connection to it using a configuration file that is automatically provided on the AzureML compute instances but which [we must create](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace) if we run this notebook on our own desktop or laptop machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779e333-c885-4621-b558-f8b6c2c2912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = azureml.core.Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e4cbb",
   "metadata": {},
   "source": [
    "The environment we'll connect to is named in the `config.yml` file, and was created by the work described in UM161. It is hosted as a docker image in an AzureContainerRepository and requires no recompilation to start our run, greatly decreasing the time a run takes to begin. We can examine the environment within AzureML studio [here](https://ml.azure.com/environments/deepspeed-transformers/version/1?wsid=/subscriptions/f095ceab-5063-4790-b393-9d86584f30a6/resourcegroups/deepqna-ml/workspaces/deepqna&tid=d9f0931a-8fbd-456c-9201-3c132995507e). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a34ff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = azureml.core.Environment.get(workspace, config['environment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f2b57",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "Connect to (or create) the experiment that will host the training run we'll launch. A single experiment can host many runs, each exploring a different set of parameters, architecture, or other approach to a the same problem. Metrics from multiple runs within a single experiment can be plotted against each other in AzureML studio. We can see the experiment within AzureML studio [here](https://ml.azure.com/experiments/id/d34a50ed-0314-438e-9d3f-71e932155d45?wsid=/subscriptions/f095ceab-5063-4790-b393-9d86584f30a6/resourcegroups/deepqna-ml/workspaces/deepqna&tid=d9f0931a-8fbd-456c-9201-3c132995507e). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6800e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = azureml.core.Experiment(workspace, config['experiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b62de3-53c5-43fa-8ab5-0b7a4f8f1fc3",
   "metadata": {},
   "source": [
    "## Configure and submit the run\n",
    "\n",
    "Our run requires several configuration components. It is worth examining the relevant entries in the `sec/config.yml` to see what we are passing and the structure of `src/train.py` to see what the training script does on each node. A summary:\n",
    "\n",
    "- a distributed job config controls the underlying PyTorch parallelization process, here this means the machine and GPUs/machine counts\n",
    "- a [ScriptRunConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py) that describes the run to the AzureML run controller, where we glue together the compute target, the environment, and the training command\n",
    "- a command run by the ScriptRunConfig, here a simple call out to `src/train.py`\n",
    "- an implicit Deepspeed configuration, referenced within the `src/train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb9a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_job_config = azureml.core.runconfig.PyTorchConfiguration(**config['pytorch_configuration'])\n",
    "aml_config = azureml.core.ScriptRunConfig(\n",
    "             source_directory=config['source_directory'],\n",
    "             command=config['training_command'],\n",
    "             environment=environment,\n",
    "             compute_target=config['compute_target'],\n",
    "             distributed_job_config=distributed_job_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5479712",
   "metadata": {},
   "source": [
    "With the run configured we submit it and tag it with metadata tags that will be helpful in understanding it later. Each of these tags is discoverable within the `src/config.yml` which will be included with the run's output, but by writing them here as well they become visible within the AzureML studio interface. We link to the AzureML studio entry for this run in the url below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(aml_config)\n",
    "run.set_tags({\n",
    "    \"model\":config['model'], \n",
    "    \"task\":config['task'],\n",
    "    \"metric\":config['metric'],\n",
    "    \"environment\":config['environment'],\n",
    "    \"num_train_epochs\":str(config['training_args']['num_train_epochs']),\n",
    "    \"batch_size\":str(config['training_args']['per_device_train_batch_size'])\n",
    "})\n",
    "\n",
    "print(f\"View run details:\\n{run.get_portal_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed5813",
   "metadata": {},
   "source": [
    "## Working with the run output\n",
    "\n",
    "We can wait for that run to complete, checking AzureML studio or calling  `run.get_status()` to verify that it is done, but here we'll look at a cached run so we don't have to wait at all to see what the results of our job above might be. Like when Julia Child would already have a souffle done so you got to see the results right away.\n",
    "\n",
    "We'll load our prior run by its ID, an immutable unique identifier, and provide its final performance and a link to its entry in AzureML studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4930a3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_run = workspace.get_run('training-quickstart_1628401731_e4461019')\n",
    "\n",
    "print(f\"Ending {config['metric']}: {prior_run.get_metrics()['eval_'+config['metric']][-1]:.2f}\")\n",
    "print(f\"View prior run details:\\n{prior_run.get_portal_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df54a3",
   "metadata": {},
   "source": [
    "We can download the model that was registered by that run. The model name and version, uniquely identifying it, are metadata tags associated with the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1455551",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_aml_model = azureml.core.Model(workspace,\n",
    "                                     name=prior_run.tags['registered_model_name'],\n",
    "                                     version=prior_run.tags['registered_model_version']\n",
    "                                    )\n",
    "prior_aml_model.download(exist_ok=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5528055",
   "metadata": {},
   "source": [
    "With the model copied to our local drive, we can convert it to ONNX format, push it to the Triton model serving container, ready it for distillation, or perform some testing inference with it as we do now. We load the model and the associated tokenizer using the transformers library that wrote them out in `src/train.py` and chain them into a classification pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"model\")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"model\", id2label=['negative', 'positive'])\n",
    "pipeline = transformers.TextClassificationPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35075f-6299-4207-a2bc-586ad2ea8956",
   "metadata": {},
   "source": [
    "The task this model was trained on, [CoLA](https://nyu-mll.github.io/CoLA/), attempts to classify sentences as grammatical or ungrammatical. We'll feed it some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80bd5b-353d-4d28-ab23-71d196132e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = (\"i like pie\", \"books sent other the students\")\n",
    "for sentence in sentences:\n",
    "    print(f\"Is '{sentence}' grammatical?: {pipeline(sentence)[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee0e5a7-a334-45af-a9a6-a8735acd5cf3",
   "metadata": {},
   "source": [
    "In this notebook we've:\n",
    "\n",
    "- connected to an AzureML workspace\n",
    "- started a training run using a specified training script and environment\n",
    "- retrieved results from that run\n",
    "- used those results to perform some inferential steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f52aa-3f17-49b3-86e6-46d9114c4d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aml]",
   "language": "python",
   "name": "conda-env-aml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
